#-------------------------------------------------------------------------------
Paper: Nesterov Accelerated Shuffling Gradient Method for Convex Optimization
Our code (and updates) can be found at https://github.com/htt-trangtran/nasg

#-------------------------------------------------------------------------------

Our code consists of two folders: Neural_Network and Logistic_Regression, where
you will find the full code for the respective models. If you want to implement 
a short version of every algorithm, you may choose to run file run_demo.py in the
Logistic_Regression folder.

Our run_demo.py file implements all algorithms using w8a dataset for ONLY 3 EPOCHS.
Then the results are plotted automatically. You can modify the code if more epochs 
is needed. 

In the other hand, you can start with the run.py files in each folder, which contain
the hyper-parameter tuning setting for each of our experiments. Our algorithms can be 
found in the Python files: algorithms.py.

#-------------------------------------------------------------------------------
